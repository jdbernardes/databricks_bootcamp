{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98a375d6-12a5-4523-8ec3-fd65da42c2cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# -- Tabela de origem: Lê os arquivos JSON do bucket S3\n",
    "# CREATE OR REFRESH STREAMING LIVE TABLE bronze_customers\n",
    "# COMMENT \"Dados brutos dos novos clientes do bucket S3\"\n",
    "# AS SELECT *\n",
    "# FROM cloud_files(\n",
    "#     \"s3a://databricks-bootcamp-julio\",\n",
    "#     \"json\",\n",
    "#     MAP(\n",
    "#         \"inferSchema\", \"false\",\n",
    "#         \"multiline\", \"true\",\n",
    "#         \"cloudFiles.schemaHints\", \"customer_id STRING, name STRING, email STRING, btc_balance DOUBLE, usd_balance DOUBLE, last_update TIMESTAMP\",\n",
    "#         \"cloudFiles.schemaLocation\", \"/FileStore/autoloader_schemas/bronze_customers\"\n",
    "#     )\n",
    "# );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55ea36bd-4032-40b3-ab2d-363278191aca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importa os tipos de dados necessários do PySpark SQL\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "# Define o caminho do seu bucket S3\n",
    "# Certifique-se de que este bucket está acessível pelo seu cluster Databricks.\n",
    "# Isso geralmente é feito através de roles IAM (AWS) ou credenciais de acesso (Azure/GCP)\n",
    "# configuradas no cluster.\n",
    "s3_bucket_path = \"s3a://databricks-bootcamp-julio/\"\n",
    "\n",
    "# 1. Definição explícita do esquema dos seus dados JSON\n",
    "# É crucial definir o esquema explicitamente para garantir que os dados sejam lidos corretamente\n",
    "# e para evitar a inferência de esquema (que pode ser inconsistente em datasets grandes ou com dados sujos).\n",
    "# Baseado na sua definição anterior:\n",
    "# customer_id STRING, name STRING, email STRING, btc_balance DOUBLE, usd_balance DOUBLE, last_update TIMESTAMP\n",
    "\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),   # customer_id como String\n",
    "    StructField(\"name\", StringType(), True),          # name como String\n",
    "    StructField(\"email\", StringType(), True),         # email como String\n",
    "    StructField(\"btc_balance\", DoubleType(), True),   # btc_balance como Double\n",
    "    StructField(\"usd_balance\", DoubleType(), True),   # usd_balance como Double\n",
    "    StructField(\"last_update\", TimestampType(), True) # last_update como Timestamp\n",
    "])\n",
    "\n",
    "# 2. Leitura dos arquivos JSON do S3 em modo batch\n",
    "# Usamos spark.read para operações em lote.\n",
    "# .option(\"multiline\", \"true\") é importante se cada arquivo JSON contém um único objeto JSON formatado em várias linhas.\n",
    "# .schema(customer_schema) aplica o esquema que definimos.\n",
    "print(f\"Lendo arquivos JSON do caminho S3: {s3_bucket_path}\")\n",
    "try:\n",
    "    df_customers = spark.read \\\n",
    "        .format(\"json\") \\\n",
    "        .option(\"multiline\", \"true\") \\\n",
    "        .schema(customer_schema) \\\n",
    "        .load(s3_bucket_path)\n",
    "\n",
    "    print(\"Pré-visualização dos dados lidos:\")\n",
    "    df_customers.show(5, truncate=False) # Mostra as 5 primeiras linhas sem truncar o conteúdo\n",
    "    df_customers.printSchema() # Mostra o esquema inferido (que será o que definimos)\n",
    "\n",
    "    # 3. Salvando os dados em uma tabela Delta Lake chamada 'bronze_customers'\n",
    "    # Usamos o modo 'overwrite' para recriar a tabela a cada execução, ou 'append' para adicionar novos dados.\n",
    "    # Para a primeira carga, 'overwrite' é comum. Se for reexecutar e quiser manter os dados, mude para 'append'.\n",
    "    table_name = \"bronze_customers\"\n",
    "    print(f\"Salvando dados na tabela Delta: {table_name}\")\n",
    "\n",
    "    # Certifique-se de ter um catálogo e schema padrão selecionados ou especifique-os:\n",
    "    # Por exemplo: spark.sql(\"USE CATALOG default;\") ou spark.sql(\"USE default.my_schema;\")\n",
    "    # Em DLT, ele geralmente usa o catálogo/schema do pipeline. Para notebooks, pode ser 'default.default'.\n",
    "\n",
    "    # Para garantir que você está escrevendo para o Hive Metastore ou Unity Catalog (se habilitado)\n",
    "    # use saveAsTable.\n",
    "    df_customers.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(table_name)\n",
    "\n",
    "    print(f\"Dados salvos com sucesso na tabela Delta '{table_name}'.\")\n",
    "\n",
    "    # 4. Verificando a tabela Delta criada\n",
    "    print(f\"Verificando os dados na tabela '{table_name}':\")\n",
    "    display(spark.sql(f\"SELECT * FROM {table_name} LIMIT 10\"))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao processar os dados: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3906b661-ba32-4da1-aae4-71abac907d18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Customer Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
